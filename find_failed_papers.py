#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
æŸ¥æ‰¾åµŒå…¥ç”Ÿæˆå¤±è´¥çš„è®ºæ–‡IDå·¥å…·
æ”¯æŒå¤šç§æ–¹æ³•ï¼š
1. å¯¹æ¯”åŸå§‹å…ƒæ•°æ®å’ŒH5æ–‡ä»¶ï¼Œæ‰¾å‡ºç¼ºå¤±çš„è®ºæ–‡
2. åˆ†ææ—¥å¿—æ–‡ä»¶ï¼Œæå–å¤±è´¥çš„è®ºæ–‡ID
3. ç”Ÿæˆé‡æ–°å¤„ç†çš„è„šæœ¬

ä½¿ç”¨ç¤ºä¾‹ï¼š

1. ä»…åˆ†ææ—¥å¿—æ–‡ä»¶ï¼ˆæœ€å¿«ï¼Œä¸éœ€è¦é¢„åŠ è½½ï¼‰ï¼š
   python find_failed_papers.py \
       --arxiv_metadata data/arxiv/arxiv-metadata-oai-snapshot.json \
       --log_files logs/tei_embedding_generation_*.log

2. å¯¹æ¯”H5æ–‡ä»¶å’ŒåŸå§‹æ•°æ®ï¼š
   python find_failed_papers.py \
       --arxiv_metadata data/arxiv/arxiv-metadata-oai-snapshot.json \
       --h5_file data/arxiv/embeddings/arxiv_embeddings_20241201_143022.h5

3. ç»¼åˆåˆ†æï¼ˆæ—¥å¿—+H5å¯¹æ¯”ï¼‰å¹¶è·å–è¯¦ç»†ä¿¡æ¯ï¼š
   python find_failed_papers.py \
       --arxiv_metadata data/arxiv/arxiv-metadata-oai-snapshot.json \
       --h5_file data/arxiv/embeddings/arxiv_embeddings_20241201_143022.h5 \
       --log_files logs/tei_embedding_generation_*.log \
       --get_details \
       --generate_retry_script

4. åªä»æ—¥å¿—æå–å¤±è´¥IDå¹¶ç”Ÿæˆé‡è¯•è„šæœ¬ï¼š
   python find_failed_papers.py \
       --arxiv_metadata data/arxiv/arxiv-metadata-oai-snapshot.json \
       --log_files logs/tei_embedding_generation_*.log \
       --generate_retry_script
"""

import json
import h5py
import re
import os
import argparse
import logging
from datetime import datetime
from tqdm import tqdm
import pandas as pd


def setup_logger():
    """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(f'failed_papers_analysis_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)


class FailedPapersFinder:
    """å¤±è´¥è®ºæ–‡æŸ¥æ‰¾å™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def get_original_paper_ids(self, arxiv_metadata_file, start_idx=0, max_samples=None):
        """ä»åŸå§‹arXivå…ƒæ•°æ®æ–‡ä»¶ä¸­è·å–æ‰€æœ‰è®ºæ–‡ID"""
        self.logger.info(f"æ­£åœ¨è¯»å–åŸå§‹å…ƒæ•°æ®æ–‡ä»¶: {arxiv_metadata_file}")
        
        paper_ids = []
        count = 0
        processed = 0
        
        with open(arxiv_metadata_file, 'r', encoding='utf-8') as f:
            # è·³è¿‡å‰é¢çš„è¡Œ
            for _ in range(start_idx):
                next(f, None)
                count += 1
            
            for line in tqdm(f, desc="è¯»å–åŸå§‹æ•°æ®"):
                try:
                    paper = json.loads(line)
                    count += 1
                    
                    # æ£€æŸ¥å¿…è¦å­—æ®µï¼ˆä¸åŸå§‹è„šæœ¬ä¿æŒä¸€è‡´ï¼‰
                    if 'title' in paper and 'abstract' in paper and paper['title'] and paper['abstract']:
                        paper_id = paper.get('id', '')
                        if paper_id:
                            paper_ids.append(paper_id)
                            processed += 1
                            
                            if max_samples and processed >= max_samples:
                                break
                                
                except json.JSONDecodeError:
                    continue
        
        self.logger.info(f"åŸå§‹æ•°æ®ä¸­ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡æ•°é‡: {len(paper_ids):,}")
        return paper_ids
    
    def get_h5_paper_ids(self, h5_file_path):
        """ä»H5æ–‡ä»¶ä¸­è·å–æ‰€æœ‰è®ºæ–‡ID"""
        self.logger.info(f"æ­£åœ¨è¯»å–H5æ–‡ä»¶: {h5_file_path}")
        
        with h5py.File(h5_file_path, 'r') as h5_file:
            paper_ids = h5_file['paper_ids'][:]
            # è§£ç ä¸ºå­—ç¬¦ä¸²
            paper_ids = [pid.decode('utf-8') for pid in paper_ids]
        
        self.logger.info(f"H5æ–‡ä»¶ä¸­çš„è®ºæ–‡æ•°é‡: {len(paper_ids):,}")
        return paper_ids
    
    def compare_paper_ids(self, original_ids, h5_ids):
        """å¯¹æ¯”åŸå§‹æ•°æ®å’ŒH5æ–‡ä»¶ä¸­çš„è®ºæ–‡IDï¼Œæ‰¾å‡ºç¼ºå¤±çš„"""
        self.logger.info("å¼€å§‹å¯¹æ¯”è®ºæ–‡ID...")
        
        original_set = set(original_ids)
        h5_set = set(h5_ids)
        
        # æ‰¾å‡ºç¼ºå¤±çš„è®ºæ–‡ID
        missing_ids = original_set - h5_set
        
        # æ‰¾å‡ºé¢å¤–çš„è®ºæ–‡IDï¼ˆç†è®ºä¸Šä¸åº”è¯¥æœ‰ï¼‰
        extra_ids = h5_set - original_set
        
        stats = {
            'åŸå§‹æ•°æ®è®ºæ–‡æ•°': len(original_set),
            'H5æ–‡ä»¶è®ºæ–‡æ•°': len(h5_set),
            'ç¼ºå¤±è®ºæ–‡æ•°': len(missing_ids),
            'é¢å¤–è®ºæ–‡æ•°': len(extra_ids),
            'æˆåŠŸç‡': f"{(len(h5_set) / len(original_set) * 100):.2f}%" if original_set else "0%"
        }
        
        return {
            'stats': stats,
            'missing_ids': list(missing_ids),
            'extra_ids': list(extra_ids)
        }
    
    def extract_failed_ids_from_logs(self, log_file_paths, error_patterns=None):
        """ä»æ—¥å¿—æ–‡ä»¶ä¸­æå–å¤±è´¥çš„è®ºæ–‡ID"""
        if error_patterns is None:
            error_patterns = [
                r'å¤„ç†è®ºæ–‡ ([^\s]+) æ—¶å‡ºé”™',
                r'å¤„ç†è®ºæ–‡ ([^\s]+) çš„åµŒå…¥æ—¶å‡ºé”™',
                r'è®ºæ–‡ ([^\s]+) å¤„ç†å¤±è´¥',
                r'ERROR.*è®ºæ–‡ID: ([^\s]+)',
                r'å¤±è´¥.*è®ºæ–‡.*([0-9]{4}\.[0-9]{4,5})',
            ]
        
        failed_ids = set()
        
        for log_file in log_file_paths:
            if not os.path.exists(log_file):
                self.logger.warning(f"æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {log_file}")
                continue
                
            self.logger.info(f"åˆ†ææ—¥å¿—æ–‡ä»¶: {log_file}")
            
            with open(log_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    for pattern in error_patterns:
                        matches = re.findall(pattern, line)
                        for match in matches:
                            if match.strip():
                                failed_ids.add(match.strip())
        
        self.logger.info(f"ä»æ—¥å¿—ä¸­æå–åˆ° {len(failed_ids)} ä¸ªå¤±è´¥çš„è®ºæ–‡ID")
        return list(failed_ids)
    
    def get_paper_details(self, paper_ids, arxiv_metadata_file):
        """è·å–æŒ‡å®šè®ºæ–‡IDçš„è¯¦ç»†ä¿¡æ¯"""
        self.logger.info(f"è·å– {len(paper_ids)} ä¸ªè®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯...")
        
        target_ids = set(paper_ids)
        paper_details = []
        
        with open(arxiv_metadata_file, 'r', encoding='utf-8') as f:
            for line in tqdm(f, desc="æœç´¢è®ºæ–‡è¯¦æƒ…"):
                try:
                    paper = json.loads(line)
                    paper_id = paper.get('id', '')
                    
                    if paper_id in target_ids:
                        paper_details.append({
                            'id': paper_id,
                            'title': paper.get('title', '').replace('\n', ' '),
                            'abstract': paper.get('abstract', '').replace('\n', ' '),
                            'authors': paper.get('authors', ''),
                            'categories': paper.get('categories', ''),
                            'journal_ref': paper.get('journal-ref', ''),
                            'doi': paper.get('doi', ''),
                            'update_date': paper.get('update_date', '')
                        })
                        
                        target_ids.remove(paper_id)
                        if not target_ids:  # æ‰¾åˆ°æ‰€æœ‰ç›®æ ‡è®ºæ–‡å°±åœæ­¢
                            break
                            
                except json.JSONDecodeError:
                    continue
        
        self.logger.info(f"æ‰¾åˆ° {len(paper_details)} ä¸ªè®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯")
        return paper_details
    
    def generate_retry_script(self, failed_paper_details, original_script_args, output_file):
        """ç”Ÿæˆé‡æ–°å¤„ç†å¤±è´¥è®ºæ–‡çš„è„šæœ¬"""
        self.logger.info(f"ç”Ÿæˆé‡è¯•è„šæœ¬: {output_file}")
        
        # åˆ›å»ºå¤±è´¥è®ºæ–‡çš„ä¸´æ—¶JSONæ–‡ä»¶
        temp_json_file = "failed_papers_temp.json"
        
        script_content = f"""#!/bin/bash
# é‡æ–°å¤„ç†å¤±è´¥è®ºæ–‡çš„è„šæœ¬
# ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
# å¤±è´¥è®ºæ–‡æ•°é‡: {len(failed_paper_details)}

echo "å¼€å§‹é‡æ–°å¤„ç† {len(failed_paper_details)} ä¸ªå¤±è´¥çš„è®ºæ–‡..."

# åˆ›å»ºä¸´æ—¶çš„å¤±è´¥è®ºæ–‡æ•°æ®æ–‡ä»¶
cat > {temp_json_file} << 'EOF'
"""
        
        # æ·»åŠ å¤±è´¥è®ºæ–‡çš„JSONæ•°æ®
        for paper in failed_paper_details:
            script_content += json.dumps(paper, ensure_ascii=False) + "\n"
        
        script_content += f"""EOF

# è¿è¡ŒåµŒå…¥ç”Ÿæˆè„šæœ¬
python generate_embeddings_tei.py \\
    --input_file {temp_json_file} \\
    --output_dir {original_script_args.get('output_dir', 'data/arxiv/embeddings')} \\
    --log_dir {original_script_args.get('log_dir', 'logs')} \\
    --tei_url {original_script_args.get('tei_url', 'http://127.0.0.1:8080/embed')} \\
    --batch_size {original_script_args.get('batch_size', 50)} \\
    --max_concurrent {original_script_args.get('max_concurrent', 20)} \\
    --memory_limit_mb {original_script_args.get('memory_limit_mb', 2048)} \\
    --start_idx 0 \\
    --max_samples {len(failed_paper_details)} \\
    --log_level INFO \\
    {f"--prompt_name {original_script_args['prompt_name']}" if original_script_args.get('prompt_name') else ""}

echo "é‡æ–°å¤„ç†å®Œæˆ!"

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
rm -f {temp_json_file}
"""
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(script_content)
        
        # ç»™è„šæœ¬æ·»åŠ æ‰§è¡Œæƒé™
        os.chmod(output_file, 0o755)
        
        self.logger.info(f"é‡è¯•è„šæœ¬å·²ç”Ÿæˆ: {output_file}")
        return output_file


def save_results(results, output_dir):
    """ä¿å­˜åˆ†æç»“æœ"""
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    if 'comparison' in results:
        with open(os.path.join(output_dir, 'comparison_stats.json'), 'w', encoding='utf-8') as f:
            json.dump(results['comparison'], f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜ç¼ºå¤±çš„è®ºæ–‡ID
    if 'missing_ids' in results:
        with open(os.path.join(output_dir, 'missing_paper_ids.txt'), 'w', encoding='utf-8') as f:
            for paper_id in results['missing_ids']:
                f.write(f"{paper_id}\n")
        
        # ä¿å­˜ä¸ºJSONæ ¼å¼
        with open(os.path.join(output_dir, 'missing_paper_ids.json'), 'w', encoding='utf-8') as f:
            json.dump(results['missing_ids'], f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜ä»æ—¥å¿—æå–çš„å¤±è´¥ID
    if 'log_failed_ids' in results:
        with open(os.path.join(output_dir, 'log_failed_ids.txt'), 'w', encoding='utf-8') as f:
            for paper_id in results['log_failed_ids']:
                f.write(f"{paper_id}\n")
        
        with open(os.path.join(output_dir, 'log_failed_ids.json'), 'w', encoding='utf-8') as f:
            json.dump(results['log_failed_ids'], f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜å¤±è´¥è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯
    if 'failed_paper_details' in results:
        with open(os.path.join(output_dir, 'failed_papers_details.json'), 'w', encoding='utf-8') as f:
            json.dump(results['failed_paper_details'], f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜ä¸ºCSVæ ¼å¼ä¾¿äºæŸ¥çœ‹
        if results['failed_paper_details']:
            df = pd.DataFrame(results['failed_paper_details'])
            df.to_csv(os.path.join(output_dir, 'failed_papers_details.csv'), index=False, encoding='utf-8')


def print_summary(results):
    """æ‰“å°ç»“æœæ‘˜è¦"""
    print("\n" + "="*60)
    print(" å¤±è´¥è®ºæ–‡åˆ†æç»“æœæ‘˜è¦")
    print("="*60)
    
    if 'comparison' in results:
        comparison = results['comparison']
        print(f"\nğŸ“Š æ•°æ®å¯¹æ¯”ç»Ÿè®¡:")
        for key, value in comparison['stats'].items():
            print(f"  {key}: {value}")
    
    if 'missing_ids' in results:
        print(f"\nâŒ ç¼ºå¤±è®ºæ–‡æ•°é‡: {len(results['missing_ids'])}")
        if results['missing_ids'] and len(results['missing_ids']) <= 10:
            print("  ç¤ºä¾‹ID:")
            for paper_id in results['missing_ids'][:10]:
                print(f"    - {paper_id}")
        elif len(results['missing_ids']) > 10:
            print("  ç¤ºä¾‹ID (å‰10ä¸ª):")
            for paper_id in results['missing_ids'][:10]:
                print(f"    - {paper_id}")
            print(f"    ... è¿˜æœ‰ {len(results['missing_ids']) - 10} ä¸ª")
    
    if 'log_failed_ids' in results:
        print(f"\nğŸ“‹ æ—¥å¿—ä¸­å‘ç°çš„å¤±è´¥è®ºæ–‡: {len(results['log_failed_ids'])}")
        if results['log_failed_ids'] and len(results['log_failed_ids']) <= 5:
            print("  å¤±è´¥ID:")
            for paper_id in results['log_failed_ids'][:5]:
                print(f"    - {paper_id}")
    
    if 'retry_script' in results:
        print(f"\nğŸ”„ é‡è¯•è„šæœ¬å·²ç”Ÿæˆ: {results['retry_script']}")
        print("  è¿è¡Œæ–¹å¼: bash " + results['retry_script'])
    
    print("="*60)


def main():
    parser = argparse.ArgumentParser(description="æŸ¥æ‰¾åµŒå…¥ç”Ÿæˆå¤±è´¥çš„è®ºæ–‡ID")
    parser.add_argument("--arxiv_metadata", type=str, required=True, help="åŸå§‹arXivå…ƒæ•°æ®JSONæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--h5_file", type=str, help="ç”Ÿæˆçš„H5åµŒå…¥æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--log_files", nargs="+", help="æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼ˆå¯ä»¥æŒ‡å®šå¤šä¸ªï¼‰")
    parser.add_argument("--output_dir", type=str, default="failed_papers_analysis", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--start_idx", type=int, default=0, help="åŸå§‹æ•°æ®çš„èµ·å§‹ç´¢å¼•")
    parser.add_argument("--max_samples", type=int, default=None, help="åŸå§‹æ•°æ®çš„æœ€å¤§æ ·æœ¬æ•°")
    parser.add_argument("--generate_retry_script", action='store_true', help="ç”Ÿæˆé‡è¯•è„šæœ¬")
    parser.add_argument("--original_script_args", type=str, help="åŸå§‹è„šæœ¬å‚æ•°çš„JSONæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--get_details", action='store_true', help="è·å–å¤±è´¥è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯ï¼ˆéœ€è¦è¯»å–åŸå§‹æ–‡ä»¶ï¼‰")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logger()
    
    # åˆ›å»ºæŸ¥æ‰¾å™¨
    finder = FailedPapersFinder()
    results = {}
    
    try:
        # åˆ¤æ–­æ˜¯å¦éœ€è¦é¢„åŠ è½½åŸå§‹æ•°æ®
        need_original_data = (
            args.h5_file and os.path.exists(args.h5_file)  # éœ€è¦ä¸H5æ–‡ä»¶å¯¹æ¯”
            or args.get_details                             # éœ€è¦è·å–è®ºæ–‡è¯¦æƒ…
            or args.generate_retry_script                   # éœ€è¦ç”Ÿæˆé‡è¯•è„šæœ¬
        )
        
        original_ids = None
        if need_original_data:
            logger.info("æ­¥éª¤ 1: è·å–åŸå§‹æ•°æ®ä¸­çš„è®ºæ–‡ID...")
            original_ids = finder.get_original_paper_ids(
                args.arxiv_metadata, 
                args.start_idx, 
                args.max_samples
            )
        else:
            logger.info("è·³è¿‡åŸå§‹æ•°æ®é¢„åŠ è½½ï¼ˆä»…åˆ†ææ—¥å¿—æ–‡ä»¶ï¼‰")
        
        # 2. å¦‚æœæä¾›äº†H5æ–‡ä»¶ï¼Œè¿›è¡Œå¯¹æ¯”
        if args.h5_file and os.path.exists(args.h5_file):
            if original_ids is None:
                logger.error("éœ€è¦åŸå§‹æ•°æ®æ¥ä¸H5æ–‡ä»¶å¯¹æ¯”ï¼Œä½†æœªåŠ è½½åŸå§‹æ•°æ®")
                return
                
            logger.info("æ­¥éª¤ 2: å¯¹æ¯”H5æ–‡ä»¶...")
            h5_ids = finder.get_h5_paper_ids(args.h5_file)
            comparison_result = finder.compare_paper_ids(original_ids, h5_ids)
            results['comparison'] = comparison_result
            results['missing_ids'] = comparison_result['missing_ids']
        
        # 3. å¦‚æœæä¾›äº†æ—¥å¿—æ–‡ä»¶ï¼Œåˆ†ææ—¥å¿—
        if args.log_files:
            step_num = 2 if not need_original_data else 3
            logger.info(f"æ­¥éª¤ {step_num}: åˆ†ææ—¥å¿—æ–‡ä»¶...")
            log_failed_ids = finder.extract_failed_ids_from_logs(args.log_files)
            results['log_failed_ids'] = log_failed_ids
        
        # 4. åˆå¹¶æ‰€æœ‰å¤±è´¥çš„ID
        all_failed_ids = set()
        if 'missing_ids' in results:
            all_failed_ids.update(results['missing_ids'])
        if 'log_failed_ids' in results:
            all_failed_ids.update(results['log_failed_ids'])
        
        all_failed_ids = list(all_failed_ids)
        
        # 5. è·å–å¤±è´¥è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯ï¼ˆä»…åœ¨éœ€è¦æ—¶ï¼‰
        if all_failed_ids and (args.get_details or args.generate_retry_script):
            if original_ids is None:
                logger.warning("æ— æ³•è·å–è®ºæ–‡è¯¦æƒ…ï¼šæœªåŠ è½½åŸå§‹æ•°æ®ã€‚è¯·æ·»åŠ  --get_details å‚æ•°ã€‚")
            else:
                step_num = len([x for x in [need_original_data, args.h5_file, args.log_files] if x]) + 1
                logger.info(f"æ­¥éª¤ {step_num}: è·å–å¤±è´¥è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯...")
                failed_details = finder.get_paper_details(all_failed_ids, args.arxiv_metadata)
                results['failed_paper_details'] = failed_details
                
                # 6. ç”Ÿæˆé‡è¯•è„šæœ¬
                if args.generate_retry_script and failed_details:
                    logger.info(f"æ­¥éª¤ {step_num + 1}: ç”Ÿæˆé‡è¯•è„šæœ¬...")
                    
                    # è¯»å–åŸå§‹è„šæœ¬å‚æ•°
                    original_script_args = {}
                    if args.original_script_args and os.path.exists(args.original_script_args):
                        with open(args.original_script_args, 'r', encoding='utf-8') as f:
                            original_script_args = json.load(f)
                    
                    retry_script = finder.generate_retry_script(
                        failed_details,
                        original_script_args,
                        os.path.join(args.output_dir, 'retry_failed_papers.sh')
                    )
                    results['retry_script'] = retry_script
        
        # 7. ä¿å­˜ç»“æœ
        logger.info("ä¿å­˜åˆ†æç»“æœ...")
        save_results(results, args.output_dir)
        
        # 8. æ‰“å°æ‘˜è¦
        print_summary(results)
        
        logger.info(f"åˆ†æå®Œæˆ! æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {args.output_dir}")
        
        # å¦‚æœåªåˆ†æäº†æ—¥å¿—ï¼Œç»™ç”¨æˆ·æç¤º
        if not need_original_data and 'log_failed_ids' in results:
            print(f"\nğŸ’¡ æç¤ºï¼šå¦‚æœéœ€è¦è·å–å¤±è´¥è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯æˆ–ç”Ÿæˆé‡è¯•è„šæœ¬ï¼Œè¯·æ·»åŠ ä»¥ä¸‹å‚æ•°ï¼š")
            print(f"   --get_details           # è·å–è®ºæ–‡è¯¦ç»†ä¿¡æ¯")
            print(f"   --generate_retry_script # ç”Ÿæˆé‡è¯•è„šæœ¬")
        
    except Exception as e:
        logger.error(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        raise


if __name__ == "__main__":
    main() 
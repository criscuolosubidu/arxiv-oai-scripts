#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
arXivåµŒå…¥å‘é‡H5æ–‡ä»¶å¤„ç†å·¥å…·
åŠŸèƒ½åŒ…æ‹¬ï¼šæ–‡ä»¶åˆ†æã€ç»Ÿè®¡ä¿¡æ¯ã€è´¨é‡æ£€æŸ¥ã€æœç´¢ç­‰
"""

import h5py
import numpy as np
import json
import os
import argparse
import logging
from datetime import datetime
import pandas as pd
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.metrics.pairwise import cosine_similarity
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


def setup_logger():
    """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(f'h5_analysis_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)


class H5EmbeddingAnalyzer:
    """H5åµŒå…¥å‘é‡æ–‡ä»¶åˆ†æå™¨"""
    
    def __init__(self, h5_file_path, metadata_file_path=None):
        self.h5_file_path = h5_file_path
        self.metadata_file_path = metadata_file_path
        self.logger = logging.getLogger(__name__)
        
        # åŠ è½½å…ƒæ•°æ®
        self.metadata = None
        if metadata_file_path and os.path.exists(metadata_file_path):
            with open(metadata_file_path, 'r', encoding='utf-8') as f:
                self.metadata = json.load(f)
        
        # æ‰“å¼€H5æ–‡ä»¶
        self.h5_file = h5py.File(h5_file_path, 'r')
        self.paper_ids = self.h5_file['paper_ids'][:]
        self.title_embeddings = self.h5_file['title_embeddings']
        self.abstract_embeddings = self.h5_file['abstract_embeddings']
        
        self.num_papers = len(self.paper_ids)
        self.embedding_dim = self.title_embeddings.shape[1]
        
        self.logger.info(f"åŠ è½½H5æ–‡ä»¶: {h5_file_path}")
        self.logger.info(f"è®ºæ–‡æ•°é‡: {self.num_papers:,}")
        self.logger.info(f"åµŒå…¥ç»´åº¦: {self.embedding_dim}")
    
    def get_basic_stats(self):
        """è·å–åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯"""
        file_size_gb = os.path.getsize(self.h5_file_path) / (1024**3)
        
        stats = {
            'æ–‡ä»¶è·¯å¾„': self.h5_file_path,
            'æ–‡ä»¶å¤§å°': f"{file_size_gb:.2f} GB",
            'è®ºæ–‡æ•°é‡': f"{self.num_papers:,}",
            'åµŒå…¥ç»´åº¦': self.embedding_dim,
            'æ•°æ®ç±»å‹': str(self.title_embeddings.dtype),
            'å‹ç¼©æ–¹å¼': self.title_embeddings.compression,
            'åˆ›å»ºæ—¶é—´': self.metadata.get('embedding_info', {}).get('creation_date', 'æœªçŸ¥') if self.metadata else 'æœªçŸ¥'
        }
        
        return stats
    
    def analyze_embedding_quality(self, sample_size=1000):
        """åˆ†æåµŒå…¥å‘é‡è´¨é‡"""
        self.logger.info("å¼€å§‹åˆ†æåµŒå…¥å‘é‡è´¨é‡...")
        
        # éšæœºé‡‡æ ·
        if self.num_papers > sample_size:
            indices = np.random.choice(self.num_papers, sample_size, replace=False)
        else:
            indices = np.arange(self.num_papers)
        
        # åŠ è½½é‡‡æ ·æ•°æ®
        title_sample = self.title_embeddings[indices]
        abstract_sample = self.abstract_embeddings[indices]
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        title_stats = {
            'å‡å€¼': np.mean(title_sample),
            'æ ‡å‡†å·®': np.std(title_sample),
            'æœ€å°å€¼': np.min(title_sample),
            'æœ€å¤§å€¼': np.max(title_sample),
            'é›¶å€¼æ¯”ä¾‹': np.mean(title_sample == 0),
            'æ¨¡é•¿å‡å€¼': np.mean(np.linalg.norm(title_sample, axis=1)),
            'NaNæ•°é‡': np.sum(np.isnan(title_sample))
        }
        
        abstract_stats = {
            'å‡å€¼': np.mean(abstract_sample),
            'æ ‡å‡†å·®': np.std(abstract_sample),
            'æœ€å°å€¼': np.min(abstract_sample),
            'æœ€å¤§å€¼': np.max(abstract_sample),
            'é›¶å€¼æ¯”ä¾‹': np.mean(abstract_sample == 0),
            'æ¨¡é•¿å‡å€¼': np.mean(np.linalg.norm(abstract_sample, axis=1)),
            'NaNæ•°é‡': np.sum(np.isnan(abstract_sample))
        }
        
        # è®¡ç®—æ ‡é¢˜å’Œæ‘˜è¦åµŒå…¥çš„ç›¸ä¼¼æ€§
        similarities = []
        for i in range(len(title_sample)):
            sim = cosine_similarity(
                title_sample[i:i+1], 
                abstract_sample[i:i+1]
            )[0][0]
            similarities.append(sim)
        
        similarity_stats = {
            'å¹³å‡ç›¸ä¼¼åº¦': np.mean(similarities),
            'ç›¸ä¼¼åº¦æ ‡å‡†å·®': np.std(similarities),
            'æœ€å°ç›¸ä¼¼åº¦': np.min(similarities),
            'æœ€å¤§ç›¸ä¼¼åº¦': np.max(similarities)
        }
        
        return {
            'æ ‡é¢˜åµŒå…¥ç»Ÿè®¡': title_stats,
            'æ‘˜è¦åµŒå…¥ç»Ÿè®¡': abstract_stats,
            'æ ‡é¢˜-æ‘˜è¦ç›¸ä¼¼æ€§': similarity_stats,
            'é‡‡æ ·å¤§å°': len(indices)
        }
    
    def find_similar_papers(self, query_id, top_k=10, use_title=True):
        """æ ¹æ®è®ºæ–‡IDæŸ¥æ‰¾ç›¸ä¼¼è®ºæ–‡"""
        try:
            # æ‰¾åˆ°æŸ¥è¯¢è®ºæ–‡çš„ç´¢å¼•
            query_idx = np.where(self.paper_ids == query_id.encode('utf-8'))[0]
            if len(query_idx) == 0:
                return None, f"è®ºæ–‡ID '{query_id}' æœªæ‰¾åˆ°"
            
            query_idx = query_idx[0]
            
            # è·å–æŸ¥è¯¢å‘é‡
            if use_title:
                query_vector = self.title_embeddings[query_idx:query_idx+1]
                search_embeddings = self.title_embeddings
                search_type = "æ ‡é¢˜"
            else:
                query_vector = self.abstract_embeddings[query_idx:query_idx+1]
                search_embeddings = self.abstract_embeddings
                search_type = "æ‘˜è¦"
            
            # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆåˆ†æ‰¹å¤„ç†ä»¥èŠ‚çœå†…å­˜ï¼‰
            batch_size = 10000
            similarities = []
            
            for i in tqdm(range(0, self.num_papers, batch_size), desc=f"è®¡ç®—{search_type}ç›¸ä¼¼åº¦"):
                end_idx = min(i + batch_size, self.num_papers)
                batch_embeddings = search_embeddings[i:end_idx]
                batch_sims = cosine_similarity(query_vector, batch_embeddings)[0]
                similarities.extend(batch_sims)
            
            similarities = np.array(similarities)
            
            # è·å–æœ€ç›¸ä¼¼çš„è®ºæ–‡
            top_indices = np.argsort(similarities)[::-1][:top_k]
            
            results = []
            for idx in top_indices:
                results.append({
                    'æ’å': len(results) + 1,
                    'è®ºæ–‡ID': self.paper_ids[idx].decode('utf-8'),
                    'ç›¸ä¼¼åº¦': f"{similarities[idx]:.4f}",
                    'ç´¢å¼•': idx
                })
            
            return results, None
            
        except Exception as e:
            return None, f"æœç´¢è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}"
    
    def export_sample_data(self, output_dir, sample_size=100):
        """å¯¼å‡ºé‡‡æ ·æ•°æ®ç”¨äºè¿›ä¸€æ­¥åˆ†æ"""
        os.makedirs(output_dir, exist_ok=True)
        
        # éšæœºé‡‡æ ·
        indices = np.random.choice(self.num_papers, min(sample_size, self.num_papers), replace=False)
        
        # å¯¼å‡ºæ•°æ®
        sample_ids = [self.paper_ids[i].decode('utf-8') for i in indices]
        sample_title_embeddings = self.title_embeddings[indices]
        sample_abstract_embeddings = self.abstract_embeddings[indices]
        
        # ä¿å­˜ä¸ºnumpyæ ¼å¼
        np.save(os.path.join(output_dir, 'sample_ids.npy'), sample_ids)
        np.save(os.path.join(output_dir, 'sample_title_embeddings.npy'), sample_title_embeddings)
        np.save(os.path.join(output_dir, 'sample_abstract_embeddings.npy'), sample_abstract_embeddings)
        
        # ä¿å­˜ä¸ºCSVæ ¼å¼ï¼ˆä»…IDå’Œç»Ÿè®¡ä¿¡æ¯ï¼‰
        df_data = []
        for i, idx in enumerate(indices):
            title_norm = np.linalg.norm(sample_title_embeddings[i])
            abstract_norm = np.linalg.norm(sample_abstract_embeddings[i])
            similarity = cosine_similarity(
                sample_title_embeddings[i:i+1], 
                sample_abstract_embeddings[i:i+1]
            )[0][0]
            
            df_data.append({
                'paper_id': sample_ids[i],
                'title_embedding_norm': title_norm,
                'abstract_embedding_norm': abstract_norm,
                'title_abstract_similarity': similarity
            })
        
        df = pd.DataFrame(df_data)
        df.to_csv(os.path.join(output_dir, 'sample_stats.csv'), index=False)
        
        self.logger.info(f"é‡‡æ ·æ•°æ®å·²å¯¼å‡ºåˆ°: {output_dir}")
        return output_dir
    
    def visualize_embeddings(self, output_dir, sample_size=1000):
        """å¯è§†åŒ–åµŒå…¥å‘é‡åˆ†å¸ƒ"""
        os.makedirs(output_dir, exist_ok=True)
        
        # éšæœºé‡‡æ ·
        if self.num_papers > sample_size:
            indices = np.random.choice(self.num_papers, sample_size, replace=False)
        else:
            indices = np.arange(self.num_papers)
        
        title_sample = self.title_embeddings[indices]
        abstract_sample = self.abstract_embeddings[indices]
        
        # 1. åµŒå…¥å‘é‡æ¨¡é•¿åˆ†å¸ƒ
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        title_norms = np.linalg.norm(title_sample, axis=1)
        abstract_norms = np.linalg.norm(abstract_sample, axis=1)
        
        ax1.hist(title_norms, bins=50, alpha=0.7, label='æ ‡é¢˜åµŒå…¥')
        ax1.set_xlabel('åµŒå…¥å‘é‡æ¨¡é•¿')
        ax1.set_ylabel('é¢‘æ¬¡')
        ax1.set_title('æ ‡é¢˜åµŒå…¥å‘é‡æ¨¡é•¿åˆ†å¸ƒ')
        ax1.legend()
        
        ax2.hist(abstract_norms, bins=50, alpha=0.7, label='æ‘˜è¦åµŒå…¥', color='orange')
        ax2.set_xlabel('åµŒå…¥å‘é‡æ¨¡é•¿')
        ax2.set_ylabel('é¢‘æ¬¡')
        ax2.set_title('æ‘˜è¦åµŒå…¥å‘é‡æ¨¡é•¿åˆ†å¸ƒ')
        ax2.legend()
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'embedding_norms_distribution.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. æ ‡é¢˜-æ‘˜è¦ç›¸ä¼¼åº¦åˆ†å¸ƒ
        similarities = []
        for i in range(len(title_sample)):
            sim = cosine_similarity(title_sample[i:i+1], abstract_sample[i:i+1])[0][0]
            similarities.append(sim)
        
        plt.figure(figsize=(10, 6))
        plt.hist(similarities, bins=50, alpha=0.7, edgecolor='black')
        plt.xlabel('ä½™å¼¦ç›¸ä¼¼åº¦')
        plt.ylabel('é¢‘æ¬¡')
        plt.title(f'æ ‡é¢˜-æ‘˜è¦åµŒå…¥ç›¸ä¼¼åº¦åˆ†å¸ƒ (é‡‡æ ·å¤§å°: {len(title_sample):,})')
        plt.axvline(np.mean(similarities), color='red', linestyle='--', label=f'å¹³å‡å€¼: {np.mean(similarities):.3f}')
        plt.legend()
        plt.savefig(os.path.join(output_dir, 'title_abstract_similarity_distribution.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        # 3. PCAé™ç»´å¯è§†åŒ–
        if self.embedding_dim > 2:
            # å¯¹æ ‡é¢˜å’Œæ‘˜è¦åµŒå…¥åˆ†åˆ«è¿›è¡ŒPCA
            pca = PCA(n_components=2)
            title_pca = pca.fit_transform(title_sample)
            
            pca_abstract = PCA(n_components=2)
            abstract_pca = pca_abstract.fit_transform(abstract_sample)
            
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
            
            ax1.scatter(title_pca[:, 0], title_pca[:, 1], alpha=0.6, s=1)
            ax1.set_xlabel(f'PC1 (è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_[0]:.3f})')
            ax1.set_ylabel(f'PC2 (è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_[1]:.3f})')
            ax1.set_title('æ ‡é¢˜åµŒå…¥ PCA å¯è§†åŒ–')
            
            ax2.scatter(abstract_pca[:, 0], abstract_pca[:, 1], alpha=0.6, s=1, color='orange')
            ax2.set_xlabel(f'PC1 (è§£é‡Šæ–¹å·®: {pca_abstract.explained_variance_ratio_[0]:.3f})')
            ax2.set_ylabel(f'PC2 (è§£é‡Šæ–¹å·®: {pca_abstract.explained_variance_ratio_[1]:.3f})')
            ax2.set_title('æ‘˜è¦åµŒå…¥ PCA å¯è§†åŒ–')
            
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, 'embeddings_pca_visualization.png'), dpi=300, bbox_inches='tight')
            plt.close()
        
        self.logger.info(f"å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {output_dir}")
    
    def validate_integrity(self):
        """éªŒè¯H5æ–‡ä»¶å®Œæ•´æ€§"""
        issues = []
        
        # æ£€æŸ¥æ•°æ®å½¢çŠ¶ä¸€è‡´æ€§
        if len(self.paper_ids) != self.title_embeddings.shape[0]:
            issues.append(f"è®ºæ–‡IDæ•°é‡ ({len(self.paper_ids)}) ä¸æ ‡é¢˜åµŒå…¥æ•°é‡ ({self.title_embeddings.shape[0]}) ä¸åŒ¹é…")
        
        if len(self.paper_ids) != self.abstract_embeddings.shape[0]:
            issues.append(f"è®ºæ–‡IDæ•°é‡ ({len(self.paper_ids)}) ä¸æ‘˜è¦åµŒå…¥æ•°é‡ ({self.abstract_embeddings.shape[0]}) ä¸åŒ¹é…")
        
        if self.title_embeddings.shape[1] != self.abstract_embeddings.shape[1]:
            issues.append(f"æ ‡é¢˜åµŒå…¥ç»´åº¦ ({self.title_embeddings.shape[1]}) ä¸æ‘˜è¦åµŒå…¥ç»´åº¦ ({self.abstract_embeddings.shape[1]}) ä¸åŒ¹é…")
        
        # æ£€æŸ¥é‡å¤ID
        unique_ids = np.unique(self.paper_ids)
        if len(unique_ids) != len(self.paper_ids):
            issues.append(f"å‘ç°é‡å¤çš„è®ºæ–‡IDï¼Œå”¯ä¸€IDæ•°é‡: {len(unique_ids)}, æ€»IDæ•°é‡: {len(self.paper_ids)}")
        
        # æ£€æŸ¥NaNå€¼
        title_nan_count = np.sum(np.isnan(self.title_embeddings[:1000]))  # æ£€æŸ¥å‰1000æ¡
        abstract_nan_count = np.sum(np.isnan(self.abstract_embeddings[:1000]))
        
        if title_nan_count > 0:
            issues.append(f"æ ‡é¢˜åµŒå…¥ä¸­å‘ç° {title_nan_count} ä¸ªNaNå€¼ï¼ˆå‰1000æ¡ä¸­ï¼‰")
        if abstract_nan_count > 0:
            issues.append(f"æ‘˜è¦åµŒå…¥ä¸­å‘ç° {abstract_nan_count} ä¸ªNaNå€¼ï¼ˆå‰1000æ¡ä¸­ï¼‰")
        
        return issues
    
    def close(self):
        """å…³é—­H5æ–‡ä»¶"""
        self.h5_file.close()


def print_stats_table(stats_dict, title="ç»Ÿè®¡ä¿¡æ¯"):
    """æ‰“å°æ ¼å¼åŒ–çš„ç»Ÿè®¡è¡¨æ ¼"""
    print(f"\n{'='*50}")
    print(f" {title}")
    print(f"{'='*50}")
    
    for key, value in stats_dict.items():
        if isinstance(value, dict):
            print(f"\n{key}:")
            for sub_key, sub_value in value.items():
                if isinstance(sub_value, float):
                    print(f"  {sub_key}: {sub_value:.6f}")
                else:
                    print(f"  {sub_key}: {sub_value}")
        else:
            print(f"{key}: {value}")
    print(f"{'='*50}")


def main():
    parser = argparse.ArgumentParser(description="arXivåµŒå…¥å‘é‡H5æ–‡ä»¶åˆ†æå·¥å…·")
    parser.add_argument("--h5_file", type=str, required=True, help="H5åµŒå…¥æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--metadata_file", type=str, help="å…ƒæ•°æ®JSONæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_dir", type=str, default="h5_analysis_output", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--action", type=str, choices=['stats', 'quality', 'search', 'export', 'visualize', 'validate', 'all'], 
                        default='all', help="æ‰§è¡Œçš„æ“ä½œ")
    parser.add_argument("--query_id", type=str, help="æœç´¢ç›¸ä¼¼è®ºæ–‡çš„æŸ¥è¯¢ID")
    parser.add_argument("--top_k", type=int, default=10, help="è¿”å›æœ€ç›¸ä¼¼çš„Kç¯‡è®ºæ–‡")
    parser.add_argument("--use_title", action='store_true', help="ä½¿ç”¨æ ‡é¢˜åµŒå…¥è¿›è¡Œæœç´¢ï¼ˆé»˜è®¤ä½¿ç”¨æ‘˜è¦ï¼‰")
    parser.add_argument("--sample_size", type=int, default=1000, help="åˆ†æå’Œå¯è§†åŒ–çš„é‡‡æ ·å¤§å°")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logger()
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.h5_file):
        logger.error(f"H5æ–‡ä»¶ä¸å­˜åœ¨: {args.h5_file}")
        return
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = H5EmbeddingAnalyzer(args.h5_file, args.metadata_file)
    
    try:
        os.makedirs(args.output_dir, exist_ok=True)
        
        # æ‰§è¡Œä¸åŒçš„æ“ä½œ
        if args.action in ['stats', 'all']:
            logger.info("è·å–åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯...")
            stats = analyzer.get_basic_stats()
            print_stats_table(stats, "H5æ–‡ä»¶åŸºæœ¬ä¿¡æ¯")
            
            # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
            with open(os.path.join(args.output_dir, 'basic_stats.json'), 'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=2)
        
        if args.action in ['quality', 'all']:
            logger.info("åˆ†æåµŒå…¥å‘é‡è´¨é‡...")
            quality_stats = analyzer.analyze_embedding_quality(args.sample_size)
            print_stats_table(quality_stats, "åµŒå…¥å‘é‡è´¨é‡åˆ†æ")
            
            # ä¿å­˜è´¨é‡åˆ†æç»“æœ
            with open(os.path.join(args.output_dir, 'quality_analysis.json'), 'w', encoding='utf-8') as f:
                json.dump(quality_stats, f, ensure_ascii=False, indent=2)
        
        if args.action in ['validate', 'all']:
            logger.info("éªŒè¯æ–‡ä»¶å®Œæ•´æ€§...")
            issues = analyzer.validate_integrity()
            if issues:
                print(f"\nâŒ å‘ç° {len(issues)} ä¸ªå®Œæ•´æ€§é—®é¢˜:")
                for i, issue in enumerate(issues, 1):
                    print(f"  {i}. {issue}")
            else:
                print("\nâœ… æ–‡ä»¶å®Œæ•´æ€§éªŒè¯é€šè¿‡")
            
            # ä¿å­˜éªŒè¯ç»“æœ
            with open(os.path.join(args.output_dir, 'integrity_check.json'), 'w', encoding='utf-8') as f:
                json.dump({'issues': issues, 'is_valid': len(issues) == 0}, f, ensure_ascii=False, indent=2)
        
        if args.action in ['search'] or (args.action == 'search' and args.query_id):
            if not args.query_id:
                logger.error("æœç´¢æ¨¡å¼éœ€è¦æä¾› --query_id å‚æ•°")
            else:
                logger.info(f"æœç´¢ä¸è®ºæ–‡ '{args.query_id}' ç›¸ä¼¼çš„è®ºæ–‡...")
                results, error = analyzer.find_similar_papers(
                    args.query_id, args.top_k, args.use_title
                )
                
                if error:
                    print(f"\nâŒ æœç´¢å¤±è´¥: {error}")
                else:
                    search_type = "æ ‡é¢˜" if args.use_title else "æ‘˜è¦"
                    print(f"\nğŸ” åŸºäº{search_type}åµŒå…¥çš„ç›¸ä¼¼è®ºæ–‡ (Top {args.top_k}):")
                    print("-" * 60)
                    for result in results:
                        print(f"{result['æ’å']:2d}. {result['è®ºæ–‡ID']} (ç›¸ä¼¼åº¦: {result['ç›¸ä¼¼åº¦']})")
                    
                    # ä¿å­˜æœç´¢ç»“æœ
                    with open(os.path.join(args.output_dir, f'search_results_{args.query_id}.json'), 'w', encoding='utf-8') as f:
                        json.dump({'query_id': args.query_id, 'search_type': search_type, 'results': results}, 
                                f, ensure_ascii=False, indent=2)
        
        if args.action in ['export', 'all']:
            logger.info("å¯¼å‡ºé‡‡æ ·æ•°æ®...")
            export_dir = analyzer.export_sample_data(
                os.path.join(args.output_dir, 'sample_data'), 
                args.sample_size
            )
            print(f"\nğŸ“¤ é‡‡æ ·æ•°æ®å·²å¯¼å‡ºåˆ°: {export_dir}")
        
        if args.action in ['visualize', 'all']:
            logger.info("ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
            analyzer.visualize_embeddings(
                os.path.join(args.output_dir, 'visualizations'), 
                args.sample_size
            )
            print(f"\nğŸ“Š å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {os.path.join(args.output_dir, 'visualizations')}")
        
        print(f"\nâœ… åˆ†æå®Œæˆ! æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {args.output_dir}")
        
    except Exception as e:
        logger.error(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        raise
    finally:
        analyzer.close()


if __name__ == "__main__":
    main() 
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
æ‰¹é‡å¯¼å…¥è„šæœ¬ - è‡ªåŠ¨åˆ†æ‰¹å¯¼å…¥å¤§é‡è®ºæ–‡æ•°æ®åˆ°Qdrant
æ”¯æŒæ–­ç‚¹ç»­ä¼ ã€è¿›åº¦ç›‘æ§ã€é”™è¯¯æ¢å¤ç­‰åŠŸèƒ½
"""

import os
import sys
import time
import argparse
import subprocess
import logging
from datetime import datetime


def setup_logger(log_dir: str = "logs") -> logging.Logger:
    """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
    os.makedirs(log_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f"batch_import_{timestamp}.log")
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    return logging.getLogger(__name__)


def run_import_batch(
    h5_file: str,
    metadata_file: str,
    start_index: int,
    batch_points: int,
    batch_size: int = 500,
    qdrant_url: str = "http://localhost:6333",
    collection_name: str = "arxiv_papers",
    recreate_collection: bool = False,
    timeout: int = 300,
    logger: logging.Logger = None
) -> bool:
    """è¿è¡Œå•æ‰¹æ¬¡å¯¼å…¥"""
    
    cmd = [
        "python", "import_to_qdrant.py",  # ä¿®æ­£æ–‡ä»¶å
        "--h5_file", h5_file,
        "--metadata_file", metadata_file,
        "--start_index", str(start_index),
        "--max_points", str(batch_points),
        "--batch_size", str(batch_size),
        "--qdrant_url", qdrant_url,
        "--collection_name", collection_name,
        "--timeout", str(timeout)
    ]
    
    # åªåœ¨éœ€è¦æ—¶æ·»åŠ recreate_collectionå‚æ•°
    if recreate_collection:
        cmd.append("--recreate_collection")
    
    logger.info(f"å¼€å§‹å¯¼å…¥æ‰¹æ¬¡: ç´¢å¼• {start_index:,} - {start_index + batch_points - 1:,}")
    logger.info(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
    
    try:
        start_time = time.time()
        result = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8')
        end_time = time.time()
        
        duration = end_time - start_time
        
        if result.returncode == 0:
            logger.info(f"æ‰¹æ¬¡å¯¼å…¥æˆåŠŸï¼Œè€—æ—¶: {duration:.2f} ç§’")
            logger.info(f"è¾“å‡º: {result.stdout}")
            return True
        else:
            logger.error(f"æ‰¹æ¬¡å¯¼å…¥å¤±è´¥ï¼Œè¿”å›ç : {result.returncode}")
            logger.error(f"é”™è¯¯è¾“å‡º: {result.stderr}")
            return False
            
    except Exception as e:
        logger.error(f"æ‰§è¡Œå¯¼å…¥å‘½ä»¤æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return False


def main():
    parser = argparse.ArgumentParser(description="æ‰¹é‡å¯¼å…¥å¤§é‡è®ºæ–‡æ•°æ®åˆ°Qdrant")
    parser.add_argument("--h5_file", type=str, required=True,
                        help="H5åµŒå…¥å‘é‡æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--metadata_file", type=str, required=True,
                        help="å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--total_papers", type=int, required=True,
                        help="æ€»è®ºæ–‡æ•°é‡")
    parser.add_argument("--batch_points", type=int, default=500000,
                        help="æ¯æ‰¹æ¬¡å¯¼å…¥çš„è®ºæ–‡æ•°é‡")
    parser.add_argument("--batch_size", type=int, default=500,
                        help="å•æ¬¡ä¸Šä¼ çš„æ‰¹é‡å¤§å°")
    parser.add_argument("--start_batch", type=int, default=0,
                        help="ä»ç¬¬å‡ æ‰¹å¼€å§‹ï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰")
    parser.add_argument("--qdrant_url", type=str, default="http://localhost:6333",
                        help="QdrantæœåŠ¡URL")
    parser.add_argument("--collection_name", type=str, default="arxiv_papers",
                        help="Qdranté›†åˆåç§°")
    parser.add_argument("--log_dir", type=str, default="logs",
                        help="æ—¥å¿—è¾“å‡ºç›®å½•")
    parser.add_argument("--continue_on_error", action="store_true",
                        help="é‡åˆ°é”™è¯¯æ—¶ç»§ç»­å¤„ç†ä¸‹ä¸€æ‰¹æ¬¡")
    parser.add_argument("--recreate_collection", action="store_true",
                        help="é‡æ–°åˆ›å»ºé›†åˆï¼ˆä»…åœ¨ç¬¬ä¸€æ‰¹æ¬¡æ—¶ç”Ÿæ•ˆï¼Œåˆ é™¤ç°æœ‰æ•°æ®ï¼‰")
    parser.add_argument("--timeout", type=int, default=300,
                        help="Qdrantå®¢æˆ·ç«¯è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤300ç§’")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logger(args.log_dir)
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.h5_file):
        logger.error(f"H5æ–‡ä»¶ä¸å­˜åœ¨: {args.h5_file}")
        return
    
    if not os.path.exists(args.metadata_file):
        logger.error(f"å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.metadata_file}")
        return
    
    # è®¡ç®—æ‰¹æ¬¡ä¿¡æ¯
    total_batches = (args.total_papers + args.batch_points - 1) // args.batch_points
    logger.info(f"æ€»è®ºæ–‡æ•°: {args.total_papers:,}")
    logger.info(f"æ¯æ‰¹æ¬¡è®ºæ–‡æ•°: {args.batch_points:,}")
    logger.info(f"æ€»æ‰¹æ¬¡æ•°: {total_batches}")
    logger.info(f"ä»ç¬¬ {args.start_batch + 1} æ‰¹å¼€å§‹")
    
    # å¼€å§‹æ‰¹é‡å¯¼å…¥
    successful_batches = 0
    failed_batches = 0
    overall_start_time = time.time()
    
    # é›†åˆå¤„ç†è¯´æ˜
    if args.recreate_collection:
        logger.warning("âš ï¸  å°†åœ¨ç¬¬ä¸€æ‰¹æ¬¡é‡æ–°åˆ›å»ºé›†åˆï¼Œè¿™ä¼šåˆ é™¤ç°æœ‰æ•°æ®ï¼")
    else:
        logger.info("âœ… å°†è¿½åŠ æ•°æ®åˆ°ç°æœ‰é›†åˆï¼ˆå¦‚æœé›†åˆä¸å­˜åœ¨ä¼šè‡ªåŠ¨åˆ›å»ºï¼‰")
    
    for batch_num in range(args.start_batch, total_batches):
        start_index = batch_num * args.batch_points
        
        # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„å®é™…è®ºæ–‡æ•°é‡
        remaining_papers = args.total_papers - start_index
        current_batch_points = min(args.batch_points, remaining_papers)
        
        if current_batch_points <= 0:
            break
        
        logger.info(f"\n{'='*60}")
        logger.info(f"å¤„ç†ç¬¬ {batch_num + 1}/{total_batches} æ‰¹æ¬¡")
        logger.info(f"ç´¢å¼•èŒƒå›´: {start_index:,} - {start_index + current_batch_points - 1:,}")
        logger.info(f"è®ºæ–‡æ•°é‡: {current_batch_points:,}")
        logger.info(f"{'='*60}")
        
        # æ‰§è¡Œå½“å‰æ‰¹æ¬¡å¯¼å…¥
        # åªåœ¨ç¬¬ä¸€æ‰¹æ¬¡ä¸”ç”¨æˆ·æŒ‡å®šæ—¶æ‰é‡æ–°åˆ›å»ºé›†åˆ
        recreate_for_this_batch = args.recreate_collection and batch_num == args.start_batch
        
        if recreate_for_this_batch:
            logger.warning(f"ğŸ”„ ç¬¬ä¸€æ‰¹æ¬¡å°†é‡æ–°åˆ›å»ºé›†åˆ '{args.collection_name}'")
        elif batch_num == args.start_batch:
            logger.info(f"ğŸ“ å°†è¿½åŠ æ•°æ®åˆ°é›†åˆ '{args.collection_name}'")
        
        success = run_import_batch(
            h5_file=args.h5_file,
            metadata_file=args.metadata_file,
            start_index=start_index,
            batch_points=current_batch_points,
            batch_size=args.batch_size,
            qdrant_url=args.qdrant_url,
            collection_name=args.collection_name,
            recreate_collection=recreate_for_this_batch,
            timeout=args.timeout,
            logger=logger
        )
        
        if success:
            successful_batches += 1
            logger.info(f"âœ… ç¬¬ {batch_num + 1} æ‰¹æ¬¡å¯¼å…¥æˆåŠŸ")
        else:
            failed_batches += 1
            logger.error(f"âŒ ç¬¬ {batch_num + 1} æ‰¹æ¬¡å¯¼å…¥å¤±è´¥")
            
            if not args.continue_on_error:
                logger.error("é‡åˆ°é”™è¯¯ï¼Œåœæ­¢å¯¼å…¥ã€‚ä½¿ç”¨ --continue_on_error å‚æ•°å¯ä»¥è·³è¿‡é”™è¯¯ç»§ç»­å¤„ç†")
                break
        
        # æ˜¾ç¤ºæ€»ä½“è¿›åº¦
        completed_papers = (batch_num + 1) * args.batch_points
        if completed_papers > args.total_papers:
            completed_papers = args.total_papers
        
        progress = (completed_papers / args.total_papers) * 100
        logger.info(f"æ€»ä½“è¿›åº¦: {completed_papers:,}/{args.total_papers:,} ({progress:.1f}%)")
        
        # çŸ­æš‚ä¼‘æ¯ï¼Œé¿å…è¿‡åº¦å ç”¨èµ„æº
        if batch_num < total_batches - 1:  # ä¸æ˜¯æœ€åä¸€æ‰¹
            logger.info("ç­‰å¾…5ç§’åç»§ç»­ä¸‹ä¸€æ‰¹æ¬¡...")
            time.sleep(5)
    
    # æ€»ç»“
    overall_end_time = time.time()
    total_duration = overall_end_time - overall_start_time
    
    logger.info(f"\n{'='*60}")
    logger.info("æ‰¹é‡å¯¼å…¥å®Œæˆ!")
    logger.info(f"æ€»è€—æ—¶: {total_duration:.2f} ç§’ ({total_duration/3600:.2f} å°æ—¶)")
    logger.info(f"æˆåŠŸæ‰¹æ¬¡: {successful_batches}")
    logger.info(f"å¤±è´¥æ‰¹æ¬¡: {failed_batches}")
    logger.info(f"æ€»æ‰¹æ¬¡: {successful_batches + failed_batches}")
    
    if failed_batches > 0:
        logger.warning(f"æœ‰ {failed_batches} ä¸ªæ‰¹æ¬¡å¯¼å…¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
    else:
        logger.info("ğŸ‰ æ‰€æœ‰æ‰¹æ¬¡å¯¼å…¥æˆåŠŸ!")


if __name__ == "__main__":
    main() 